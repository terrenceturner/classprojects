{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Count with Spark\n",
    "\n",
    "The purpose of this project is to use Spark’s filtering, mapping and reducing capabilities to implement a simple word count example that summarizes the words in Romeo and Juliet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the NLTK Stop Word\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "configuration = SparkConf().setAppName('RomeoAndJulietCounter')\\\n",
    "                .setMaster('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring a SparkContext\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(conf=configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob.utils import strip_punc\n",
    "tokenized = sc.textFile('RomeoAndJuliet.txt')\\\n",
    "            .map(lambda line: strip_punc(line, all=True).lower())\\\n",
    "            .flatMap(lambda line: line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the stop words\n",
    "filtered = tokenized.filter(lambda word: word not in stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "word_counts = filtered.map(lambda word: (word, 1)).reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locating Words with Counts Greater Than or Equal to 60\n",
    "filtered_counts = word_counts.filter(lambda item: item[1] >= 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "sorted_items = sorted(filtered_counts.collect(),\n",
    "                key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   romeo: 298\n",
      "    thou: 277\n",
      "  juliet: 178\n",
      "     thy: 170\n",
      "   nurse: 146\n",
      " capulet: 141\n",
      "    love: 136\n",
      "    thee: 135\n",
      "   shall: 110\n",
      "    lady: 109\n",
      "   friar: 104\n",
      "    come: 94\n",
      "mercutio: 83\n",
      "    good: 80\n",
      "benvolio: 79\n",
      "   enter: 75\n",
      "      go: 75\n",
      "    i’ll: 71\n",
      "  tybalt: 69\n",
      "   death: 69\n",
      "   night: 68\n",
      "lawrence: 67\n",
      "     man: 65\n",
      "    hath: 64\n",
      "     one: 60\n"
     ]
    }
   ],
   "source": [
    "# Display the results\n",
    "max_len = max([len(word) for word, count in sorted_items]) \n",
    "for word, count in sorted_items:\n",
    "    print(f'{word:>{max_len}}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
